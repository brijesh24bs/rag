{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qx4emwq7DTQ_"
      },
      "outputs": [],
      "source": [
        "#install libraries\n",
        "!pip install langchain\n",
        "!pip install langchain-groq\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libs\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "8vgGKSwNFYER"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the environment variables\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api')\n",
        "hf_api_key = userdata.get('hf_api')"
      ],
      "metadata": {
        "id": "9YVLzWJbGOM_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading pdf\n",
        "pdf =  \"https://arxiv.org/pdf/2005.11401.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(pdf)\n",
        "\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "5xKI9XvjGO8I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the docs\n",
        "docs = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0).split_documents(pages)"
      ],
      "metadata": {
        "id": "Vr9M_HDIGnWz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"You have {len(docs)} documents\")\n",
        "print(f\"The first document has {len(docs[0].page_content)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD7l3rHYGnUO",
        "outputId": "d9fb3259-bc94-4328-d336-d8def908f55f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 83 documents\n",
            "The first document has 928 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding and vectorestores\n",
        "embedding = HuggingFaceInferenceAPIEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    api_key=hf_api_key\n",
        ")\n",
        "\n",
        "db = Chroma.from_documents(\n",
        "    docs,\n",
        "    embedding,\n",
        "    persist_directory=\"./\"\n",
        ")\n",
        "\n",
        "print(db._collection.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCzR3GUDJXwl",
        "outputId": "d17c2034-e2fc-4718-e925-95044993a7b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model='llama3-70b-8192',\n",
        "    temperature=0.3,\n",
        "    api_key=groq_api_key,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "iLRmVd9UKMhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9yyQzmyLrcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "jEULopBgM908"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain({\"query\": \"What is the paper about?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf1X5gYuNuGK",
        "outputId": "c37e019b-509c-4e71-8d4e-751b96e918a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is the paper about?',\n",
              " 'result': 'The paper appears to be about a language model called RAG (Retrieval-Augmented Generator) that combines the strengths of two approaches: retrieval-based models that retrieve evidence from a database and generate text based on that evidence, and parametric models that generate text based on patterns learned from a large corpus of text. The paper presents experiments and results showing that RAG achieves high accuracy in various natural language processing tasks, such as fact verification, question answering, and text generation, and outperforms other models in some cases.'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you will see it doesn't carry the answer from the previous question\n",
        "qa_chain({\"query\": \"Write five main topics about the paper.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxxLdr7VN8rR",
        "outputId": "de8d1d4d-f60a-4b25-89be-d8d50a316cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Write five main topics about the paper.',\n",
              " 'result': 'Based on the provided context, the five main topics about the paper are:\\n\\n1. **RAG Model Performance**: The paper discusses the performance of the RAG (Retrieval-Augmented Generator) model in various tasks, including 2-way classification, generation diversity, and factual accuracy.\\n\\n2. **Comparison with Other Models**: The paper compares the performance of the RAG model with other models, such as RoBERTa and BART, in terms of accuracy, generation diversity, and factual accuracy.\\n\\n3. **Document Retrieval and Evidence**: The paper explores the ability of the RAG model to retrieve relevant documents and evidence to support its generated responses, and analyzes the overlap between the retrieved documents and gold evidence annotations.\\n\\n4. **Generation Tasks and Diversity**: The paper investigates the performance of the RAG model in generation tasks, such as Jeopardy question generation, and analyzes the diversity of the generated responses.\\n\\n5. **Factual Accuracy and Specificity**: The paper examines the factual accuracy and specificity of the responses generated by the RAG model, and compares them with those generated by other models.'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"history\",\n",
        "    return_messages=True\n",
        "  )\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Hey\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtLUI7o3OSSm",
        "outputId": "6f804584-b27d-4377-a96b-6f9e292070ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi'), AIMessage(content='Hey')]}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "35HJ9VQDOiv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "CgqNgrIJO4ph",
        "outputId": "89e028fe-9b4f-4ea3-8164-6d65b7790864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi'), AIMessage(content='Hey')]\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi there! I'm thrilled to be chatting with you. I've been trained on a vast amount of text data, including books, articles, and conversations, so I'm ready to dive into a fun conversation. By the way, did you know that I've been trained on over 1.5 billion parameters? It's a lot, I know! Anyway, what's on your mind today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.predict(input=\"What is the paper about?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "liBZesOtQWZN",
        "outputId": "6b64a05b-36d6-4372-ab0c-0fd155c508ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi'), AIMessage(content='Hey'), HumanMessage(content='Hi there!'), AIMessage(content=\"Hi there! I'm thrilled to be chatting with you. I've been trained on a vast amount of text data, including books, articles, and conversations, so I'm ready to dive into a fun conversation. By the way, did you know that I've been trained on over 1.5 billion parameters? It's a lot, I know! Anyway, what's on your mind today?\")]\n",
            "Human: What is the paper about?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I\\'m happy to share that with you! The paper that my training is based on is titled \"Language Models are Few-shot Learners\" by Tom B. Brown et al. It was published in 2020 and introduced the concept of using a large language model to perform well on a wide range of tasks with minimal fine-tuning. The paper explores the idea that large language models can be adapted to perform well on a variety of tasks, such as conversational dialogue, text classification, and language translation, with only a few examples of training data. It\\'s a fascinating read, and I\\'m proud to be a part of the next generation of language models inspired by this research!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use ConversationChain with its own PromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "{history}\n",
        "Question: {input}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "qa = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PROMPT,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "qa.predict(input=\"What is the paper about?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "t7EEDQPAQ1Ef",
        "outputId": "00817aab-bcb1-4080-f8fa-13e4e35a33e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "[HumanMessage(content='Hi'), AIMessage(content='Hey'), HumanMessage(content='Hi there!'), AIMessage(content=\"Hi there! I'm thrilled to be chatting with you. I've been trained on a vast amount of text data, including books, articles, and conversations, so I'm ready to dive into a fun conversation. By the way, did you know that I've been trained on over 1.5 billion parameters? It's a lot, I know! Anyway, what's on your mind today?\"), HumanMessage(content='What is the paper about?'), AIMessage(content='I\\'m happy to share that with you! The paper that my training is based on is titled \"Language Models are Few-shot Learners\" by Tom B. Brown et al. It was published in 2020 and introduced the concept of using a large language model to perform well on a wide range of tasks with minimal fine-tuning. The paper explores the idea that large language models can be adapted to perform well on a variety of tasks, such as conversational dialogue, text classification, and language translation, with only a few examples of training data. It\\'s a fascinating read, and I\\'m proud to be a part of the next generation of language models inspired by this research!')]\n",
            "Question: What is the paper about?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'According to the conversation, the paper \"Language Models are Few-shot Learners\" by Tom B. Brown et al. is about using large language models to perform well on a wide range of tasks with minimal fine-tuning, such as conversational dialogue, text classification, and language translation, with only a few examples of training data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converstional retrieval chain and conversational buffer memory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model='llama3-70b-8192',\n",
        "    temperature=0.3,\n",
        "    api_key=groq_api_key,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "M0WJlWwNXuPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa({\"question\": \"What is the paper about?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XveuAfUSYkmB",
        "outputId": "076f6a60-0d6e-480b-b285-fb012dc39302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\n",
            "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
            "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
            "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
            "evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\n",
            "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "\n",
            "Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\n",
            "text-to-text transformer. arXiv e-prints , 2019. URL https://arxiv.org/abs/1910.10683 .\n",
            "[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\n",
            "the parameters of a language model? arXiv e-prints , 2020. URL https://arxiv.org/abs/\n",
            "2002.08910 .\n",
            "[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\n",
            "beyond. Found. Trends Inf. Retr. , 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\n",
            "1500000019. URL https://doi.org/10.1561/1500000019 .\n",
            "[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\n",
            "Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\n",
            "ArXiv , abs/1908.09203, 2019.\n",
            "[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\n",
            "\n",
            "Document 1 : his works are considered classics of American\n",
            "literature ... His wartime experiences formed the basis for his novel\n",
            "”A Farewell to Arms” (1929) ...\n",
            "Document 2 : ... artists of the 1920s ”Lost Generation” expatriate\n",
            "community. His debut novel, ”The Sun Also Rises” , was published\n",
            "in 1926.\n",
            "BOS”\n",
            "TheSunAlsoRises”isa\n",
            "novelbythis\n",
            "authorof”A\n",
            "Farewellto\n",
            "Arms”Doc 1\n",
            "Doc 2\n",
            "Doc 3\n",
            "Doc 4\n",
            "Doc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y−i)for each generated token for input “Hem-\n",
            "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
            "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
            "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
            "responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\n",
            "Task Input Model Generation\n",
            "MS-\n",
            "MARCOdeﬁne middle\n",
            "\n",
            "generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\n",
            "This observation suggests that the generator can complete the titles without depending on speciﬁc\n",
            "documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\n",
            "ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\n",
            "Sun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\n",
            "Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\n",
            "BART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\n",
            "with \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\n",
            "how parametric and non-parametric memories work together —the non-parametric component helps\n",
            "to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\n",
            "4.4 Fact Veriﬁcation\n",
            "Human: What is the paper about?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the paper about?',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='The paper appears to be about a language model called RAG (Retrieval-Augmented Generator) that combines the strengths of retrieval-based and generation-based approaches to produce more accurate and specific responses. The model is trained to retrieve relevant documents and then generate text based on those documents. The paper presents results showing that RAG outperforms other models in various tasks, including fact verification and Jeopardy question generation.')],\n",
              " 'answer': 'The paper appears to be about a language model called RAG (Retrieval-Augmented Generator) that combines the strengths of retrieval-based and generation-based approaches to produce more accurate and specific responses. The model is trained to retrieve relevant documents and then generate text based on those documents. The paper presents results showing that RAG outperforms other models in various tasks, including fact verification and Jeopardy question generation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.Config.llm = ChatGroq(\n",
        "    model = 'llama3-8b-8192',\n",
        "    temperature = 0.3,\n",
        "    api_key = groq_api_key,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "kuKHEpXYawyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here i am explicity mentioning about the RAG Paper.  # just changes the model to llama3-8b and you can see the difference.\n",
        "qa({'question': 'Explain 5 main points of it in one sentence'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhWOYVm0YusT",
        "outputId": "359a087b-da94-40bc-b066-8f367397428e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is the paper about?\n",
            "Assistant: The paper appears to be about a language model called RAG (Retrieval-Augmented Generator) that combines the strengths of retrieval-based and generation-based approaches to produce more accurate and specific responses. The model is trained to retrieve relevant documents and then generate text based on those documents. The paper presents results showing that RAG outperforms other models in various tasks, including fact verification and Jeopardy question generation.\n",
            "Follow Up Input: Explain 5 main points of it in one sentence\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "cases for NQ, where an extractive model would score 0%.\n",
            "4.2 Abstractive Question Answering\n",
            "As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\n",
            "points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\n",
            "impressive given that (i) those models access gold passages with speciﬁc information required to\n",
            "generate the reference answer , (ii) many questions are unanswerable without the gold passages, and\n",
            "(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\n",
            "from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\n",
            "correct text more often than BART. Later, we also show that RAG generations are more diverse than\n",
            "BART generations (see §4.5).\n",
            "4.3 Jeopardy Question Generation\n",
            "Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\n",
            "\n",
            "with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\n",
            "pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\n",
            "than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\n",
            "BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\n",
            "the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\n",
            "speciﬁc by a large margin. Table 3 shows typical generations from each model.\n",
            "Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform\n",
            "best because it can generate responses that combine content from several documents. Figure 2 shows\n",
            "an example. When generating “Sun”, the posterior is high for document 2 which mentions “The\n",
            "Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\n",
            "\n",
            "documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\n",
            "mechanisms may not be necessary for RAG.\n",
            "G Parameters\n",
            "Our RAG models contain the trainable parameters for the BERT-base query and document encoder of\n",
            "DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n",
            "406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n",
            "18\n",
            "\n",
            "Document 1 : his works are considered classics of American\n",
            "literature ... His wartime experiences formed the basis for his novel\n",
            "”A Farewell to Arms” (1929) ...\n",
            "Document 2 : ... artists of the 1920s ”Lost Generation” expatriate\n",
            "community. His debut novel, ”The Sun Also Rises” , was published\n",
            "in 1926.\n",
            "BOS”\n",
            "TheSunAlsoRises”isa\n",
            "novelbythis\n",
            "authorof”A\n",
            "Farewellto\n",
            "Arms”Doc 1\n",
            "Doc 2\n",
            "Doc 3\n",
            "Doc 4\n",
            "Doc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y−i)for each generated token for input “Hem-\n",
            "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
            "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
            "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
            "responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\n",
            "Task Input Model Generation\n",
            "MS-\n",
            "MARCOdeﬁne middle\n",
            "Human: Here is the rephrased standalone question:\n",
            "\n",
            "What are the five main points of the RAG language model paper summarized in one sentence?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Explain 5 main points of it in one sentence',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='The paper appears to be about a language model called RAG (Retrieval-Augmented Generator) that combines the strengths of retrieval-based and generation-based approaches to produce more accurate and specific responses. The model is trained to retrieve relevant documents and then generate text based on those documents. The paper presents results showing that RAG outperforms other models in various tasks, including fact verification and Jeopardy question generation.'),\n",
              "  HumanMessage(content='Explain 5 main points of it in one sentence'),\n",
              "  AIMessage(content=\"I don't know. The provided context does not provide a clear summary of the five main points of the RAG language model paper. It appears to be a snippet of a research paper discussing the performance of RAG models on various tasks, but it does not provide a concise summary of the main points.\")],\n",
              " 'answer': \"I don't know. The provided context does not provide a clear summary of the five main points of the RAG language model paper. It appears to be a snippet of a research paper discussing the performance of RAG models on various tasks, but it does not provide a concise summary of the main points.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# # X = input(\"Enter first string: \").lower()\n",
        "# # Y = input(\"Enter second string: \").lower()\n",
        "# X =\"I love horror movies\"\n",
        "# Y =\"Lights out is a horror movie\"\n",
        "\n",
        "# # tokenization\n",
        "# X_list = word_tokenize(X)\n",
        "# Y_list = word_tokenize(Y)\n",
        "\n",
        "\n",
        "# # sw contains the list of stopwords\n",
        "# sw = stopwords.words('english')\n",
        "# l1 =[];l2 =[]\n",
        "\n",
        "# # remove stop words from the string\n",
        "# X_set = {w for w in X_list if not w in sw}\n",
        "# Y_set = {w for w in Y_list if not w in sw}\n",
        "\n",
        "# # form a set containing keywords of both strings\n",
        "# rvector = X_set.union(Y_set)  #{'I', 'Lights', 'horror', 'love', 'movie', 'movies'}\n",
        "\n",
        "# for w in rvector:\n",
        "#     if w in X_set: l1.append(1) # create a vector\n",
        "#     else: l1.append(0)\n",
        "#     if w in Y_set: l2.append(1)\n",
        "#     else: l2.append(0)\n",
        "# c = 0\n",
        "\n",
        "# # cosine formula\n",
        "# for i in range(len(rvector)):\n",
        "#         c+= l1[i]*l2[i]\n",
        "# cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "# print(\"similarity: \", cosine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lze1EFw9M9pu",
        "outputId": "aa0b04bc-5190-4ac7-b9a5-c082850c0022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity:  0.2886751345948129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mmr as search type and chain_type=stuff\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-8b-8192\",\n",
        "    temperature = 0.3,\n",
        "    api_key = groq_api_key,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    docs,\n",
        "    embedding,\n",
        "    persist_directory=\"./\",\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 3}),\n",
        "    memory=memory,\n",
        "    chain_type=\"stuff\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "7ZDMYujDNROL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa({\"question\": \"What is the paper about?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC-MCMmTktYP",
        "outputId": "fbca479b-c974-49b7-dd6d-7cb500e1ae0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\n",
            "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
            "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
            "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
            "evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\n",
            "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "\n",
            "generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\n",
            "This observation suggests that the generator can complete the titles without depending on speciﬁc\n",
            "documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\n",
            "ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\n",
            "Sun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\n",
            "Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\n",
            "BART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\n",
            "with \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\n",
            "how parametric and non-parametric memories work together —the non-parametric component helps\n",
            "to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\n",
            "4.4 Fact Veriﬁcation\n",
            "\n",
            "[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\n",
            "Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\n",
            "arXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\n",
            "1704.05179.\n",
            "[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\n",
            "ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\n",
            "Long Papers) , pages 889–898, Melbourne, Australia, July 2018. Association for Computational\n",
            "Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\n",
            "P18-1082 .\n",
            "[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\n",
            "Long form question answering. In Proceedings of the 57th Annual Meeting of the Association\n",
            "for Computational Linguistics , pages 3558–3567, Florence, Italy, July 2019. Association for\n",
            "Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\n",
            "Human: What is the paper about?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the paper about?',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='The paper appears to be about a natural language processing (NLP) model called RAG (Retrieval-Augmented Generation) that is designed for fact-checking and question-answering tasks. The paper presents several experiments and results that demonstrate the effectiveness of the RAG model in various NLP tasks, including fact verification, question answering, and generation of text.')],\n",
              " 'answer': 'The paper appears to be about a natural language processing (NLP) model called RAG (Retrieval-Augmented Generation) that is designed for fact-checking and question-answering tasks. The paper presents several experiments and results that demonstrate the effectiveness of the RAG model in various NLP tasks, including fact verification, question answering, and generation of text.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa({\"question\":\"Tell me 5 insights from the paper.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSk4B1_3lg-R",
        "outputId": "ee0dde5d-98d9-4fde-fd4c-113bb0d0e32b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is the paper about?\n",
            "Assistant: The paper appears to be about a natural language processing (NLP) model called RAG (Retrieval-Augmented Generation) that is designed for fact-checking and question-answering tasks. The paper presents several experiments and results that demonstrate the effectiveness of the RAG model in various NLP tasks, including fact verification, question answering, and generation of text.\n",
            "Follow Up Input: Tell me 5 insights from the paper.\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Broader Impact\n",
            "This work offers several positive societal beneﬁts over previous work: the fact that it is more\n",
            "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\n",
            "with generations that are more factual, and offers more control and interpretability. RAG could be\n",
            "employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\n",
            "with a medical index and asking it open-domain questions on that topic, or by helping people be more\n",
            "effective at their jobs.\n",
            "With these advantages also come potential downsides: Wikipedia, or any potential external knowledge\n",
            "source, will probably never be entirely factual and completely devoid of bias. Since RAG can be\n",
            "employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\n",
            "to a lesser extent, including that it might be used to generate abuse, faked or misleading content in\n",
            "\n",
            "Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\n",
            "state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\n",
            "substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\n",
            "6\n",
            "\n",
            "Document 1 : his works are considered classics of American\n",
            "literature ... His wartime experiences formed the basis for his novel\n",
            "”A Farewell to Arms” (1929) ...\n",
            "Document 2 : ... artists of the 1920s ”Lost Generation” expatriate\n",
            "community. His debut novel, ”The Sun Also Rises” , was published\n",
            "in 1926.\n",
            "BOS”\n",
            "TheSunAlsoRises”isa\n",
            "novelbythis\n",
            "authorof”A\n",
            "Farewellto\n",
            "Arms”Doc 1\n",
            "Doc 2\n",
            "Doc 3\n",
            "Doc 4\n",
            "Doc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y−i)for each generated token for input “Hem-\n",
            "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
            "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
            "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
            "responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\n",
            "Task Input Model Generation\n",
            "MS-\n",
            "MARCOdeﬁne middle\n",
            "Human: What are the 5 key insights presented in the paper about the RAG model?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Tell me 5 insights from the paper.',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='The paper appears to be about a natural language processing (NLP) model called RAG (Retrieval-Augmented Generation) that is designed for fact-checking and question-answering tasks. The paper presents several experiments and results that demonstrate the effectiveness of the RAG model in various NLP tasks, including fact verification, question answering, and generation of text.'),\n",
              "  HumanMessage(content='Tell me 5 insights from the paper.'),\n",
              "  AIMessage(content='Based on the provided context, the paper presents the following key insights about the RAG model:\\n\\n1. **Improved factual accuracy**: RAG is more strongly grounded in real factual knowledge, which makes it \"hallucinate\" less with generations that are more factual.\\n2. **Control and interpretability**: RAG offers more control and interpretability compared to previous models.\\n3. **Wide applicability**: RAG can be employed in a wide variety of scenarios with direct benefits to society, such as endowing it with a medical index and asking open-domain questions on that topic.\\n4. **Comparison to state-of-the-art models**: RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering.\\n5. **Factually accurate generation**: RAG models generate more specific and factually accurate responses, as shown in Table 3, which provides examples from generation tasks.\\n\\nThese insights highlight the advantages and capabilities of the RAG model, showcasing its potential to improve factual accuracy, control, and interpretability in various applications.')],\n",
              " 'answer': 'Based on the provided context, the paper presents the following key insights about the RAG model:\\n\\n1. **Improved factual accuracy**: RAG is more strongly grounded in real factual knowledge, which makes it \"hallucinate\" less with generations that are more factual.\\n2. **Control and interpretability**: RAG offers more control and interpretability compared to previous models.\\n3. **Wide applicability**: RAG can be employed in a wide variety of scenarios with direct benefits to society, such as endowing it with a medical index and asking open-domain questions on that topic.\\n4. **Comparison to state-of-the-art models**: RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering.\\n5. **Factually accurate generation**: RAG models generate more specific and factually accurate responses, as shown in Table 3, which provides examples from generation tasks.\\n\\nThese insights highlight the advantages and capabilities of the RAG model, showcasing its potential to improve factual accuracy, control, and interpretability in various applications.'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with mmr search_type and chain_type refine\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "  memory_key=\"chat_history\",\n",
        "  return_messages=True\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 3})\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    chain_type=\"refine\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "n170O_N4lwCg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa({'question':'What is the paper about?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWcibEeymvOz",
        "outputId": "6696167b-a13b-4f83-9999-8295b3fa6763"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Context information is below.\n",
            "------------\n",
            "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\n",
            "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
            "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
            "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
            "evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\n",
            "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "------------\n",
            "Given the context information and not prior knowledge, answer any questions\n",
            "Human: What is the paper about?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: What is the paper about?\n",
            "AI: Based on the provided context, the paper appears to be about a natural language processing (NLP) model called RAG (Retrieval-Augmented Generation) that is designed for fact-checking and evidence retrieval. The paper compares the performance of RAG with a baseline model and analyzes the quality of the retrieved evidence.\n",
            "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\n",
            "This observation suggests that the generator can complete the titles without depending on speciﬁc\n",
            "documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\n",
            "ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\n",
            "Sun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\n",
            "Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\n",
            "BART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\n",
            "with \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\n",
            "how parametric and non-parametric memories work together —the non-parametric component helps\n",
            "to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\n",
            "4.4 Fact Veriﬁcation\n",
            "------------\n",
            "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: What is the paper about?\n",
            "AI: With the additional context, I can refine the original answer.\n",
            "\n",
            "The paper appears to be about a language model, specifically BART (Bidirectional Encoder Representations from Transformers), and its ability to generate text. The paper discusses the model's performance in completing book titles and how it uses both parametric and non-parametric memories to generate text. The authors observe that the model can complete titles without relying on specific documents, suggesting that the model's parametric knowledge is sufficient to complete the titles. The paper also explores the model's ability to verify facts, which is discussed in the section \"Fact Verification\".\n",
            "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\n",
            "Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\n",
            "arXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\n",
            "1704.05179.\n",
            "[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\n",
            "ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\n",
            "Long Papers) , pages 889–898, Melbourne, Australia, July 2018. Association for Computational\n",
            "Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\n",
            "P18-1082 .\n",
            "[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\n",
            "Long form question answering. In Proceedings of the 57th Annual Meeting of the Association\n",
            "for Computational Linguistics , pages 3558–3567, Florence, Italy, July 2019. Association for\n",
            "Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\n",
            "------------\n",
            "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the paper about?',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='With the additional context, I can refine the original answer.\\n\\nThe papers listed appear to be related to natural language processing and question answering. Specifically:\\n\\n* [10] introduces SearchQA, a new Q&A dataset augmented with context from a search engine.\\n* [11] presents a hierarchical neural story generation approach.\\n* [12] discusses ELI5, a long-form question answering approach.\\n\\nThese papers are likely exploring ways to improve question answering and natural language processing capabilities.')],\n",
              " 'answer': 'With the additional context, I can refine the original answer.\\n\\nThe papers listed appear to be related to natural language processing and question answering. Specifically:\\n\\n* [10] introduces SearchQA, a new Q&A dataset augmented with context from a search engine.\\n* [11] presents a hierarchical neural story generation approach.\\n* [12] discusses ELI5, a long-form question answering approach.\\n\\nThese papers are likely exploring ways to improve question answering and natural language processing capabilities.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With MMR as search_type and Chain_type=map_rerank\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "  memory_key=\"chat_history\",\n",
        "  return_messages=True,\n",
        ")\n",
        "\n",
        "retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 3})\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    chain_type=\"map_rerank\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "a34lNlpEm231"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa({\"question\": \"What is the paper about?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3xYSrDAnr9n",
        "outputId": "9713445c-4c6d-4de3-b106-8d7f84c1d84f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapRerankDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
            "\n",
            "Question: [question here]\n",
            "Helpful Answer: [answer here]\n",
            "Score: [score between 0 and 100]\n",
            "\n",
            "How to determine the score:\n",
            "- Higher is a better answer\n",
            "- Better responds fully to the asked question, with sufficient level of detail\n",
            "- If you do not know the answer based on the context, that should be a score of 0\n",
            "- Don't be overconfident!\n",
            "\n",
            "Example #1\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Apples are red\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: red\n",
            "Score: 100\n",
            "\n",
            "Example #2\n",
            "\n",
            "Context:\n",
            "---------\n",
            "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
            "---------\n",
            "Question: what type was the car?\n",
            "Helpful Answer: a sports car or an suv\n",
            "Score: 60\n",
            "\n",
            "Example #3\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Pears are either red or orange\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: This document does not answer the question\n",
            "Score: 0\n",
            "\n",
            "Begin!\n",
            "\n",
            "Context:\n",
            "---------\n",
            "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\n",
            "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
            "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
            "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
            "evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\n",
            "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "---------\n",
            "Question: What is the paper about?\n",
            "Helpful Answer:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
            "\n",
            "Question: [question here]\n",
            "Helpful Answer: [answer here]\n",
            "Score: [score between 0 and 100]\n",
            "\n",
            "How to determine the score:\n",
            "- Higher is a better answer\n",
            "- Better responds fully to the asked question, with sufficient level of detail\n",
            "- If you do not know the answer based on the context, that should be a score of 0\n",
            "- Don't be overconfident!\n",
            "\n",
            "Example #1\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Apples are red\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: red\n",
            "Score: 100\n",
            "\n",
            "Example #2\n",
            "\n",
            "Context:\n",
            "---------\n",
            "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
            "---------\n",
            "Question: what type was the car?\n",
            "Helpful Answer: a sports car or an suv\n",
            "Score: 60\n",
            "\n",
            "Example #3\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Pears are either red or orange\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: This document does not answer the question\n",
            "Score: 0\n",
            "\n",
            "Begin!\n",
            "\n",
            "Context:\n",
            "---------\n",
            "generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\n",
            "This observation suggests that the generator can complete the titles without depending on speciﬁc\n",
            "documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\n",
            "ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\n",
            "Sun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\n",
            "Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\n",
            "BART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\n",
            "with \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\n",
            "how parametric and non-parametric memories work together —the non-parametric component helps\n",
            "to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\n",
            "4.4 Fact Veriﬁcation\n",
            "---------\n",
            "Question: What is the paper about?\n",
            "Helpful Answer:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
            "\n",
            "Question: [question here]\n",
            "Helpful Answer: [answer here]\n",
            "Score: [score between 0 and 100]\n",
            "\n",
            "How to determine the score:\n",
            "- Higher is a better answer\n",
            "- Better responds fully to the asked question, with sufficient level of detail\n",
            "- If you do not know the answer based on the context, that should be a score of 0\n",
            "- Don't be overconfident!\n",
            "\n",
            "Example #1\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Apples are red\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: red\n",
            "Score: 100\n",
            "\n",
            "Example #2\n",
            "\n",
            "Context:\n",
            "---------\n",
            "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
            "---------\n",
            "Question: what type was the car?\n",
            "Helpful Answer: a sports car or an suv\n",
            "Score: 60\n",
            "\n",
            "Example #3\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Pears are either red or orange\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: This document does not answer the question\n",
            "Score: 0\n",
            "\n",
            "Begin!\n",
            "\n",
            "Context:\n",
            "---------\n",
            "[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\n",
            "Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\n",
            "arXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\n",
            "1704.05179.\n",
            "[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\n",
            "ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\n",
            "Long Papers) , pages 889–898, Melbourne, Australia, July 2018. Association for Computational\n",
            "Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\n",
            "P18-1082 .\n",
            "[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\n",
            "Long form question answering. In Proceedings of the 57th Annual Meeting of the Association\n",
            "for Computational Linguistics , pages 3558–3567, Florence, Italy, July 2019. Association for\n",
            "Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\n",
            "---------\n",
            "Question: What is the paper about?\n",
            "Helpful Answer:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:367: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the paper about?',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='Helpful Answer: The paper is about evaluating the performance of a language model called RAG in various tasks, including 2-way classification and generation diversity.')],\n",
              " 'answer': 'Helpful Answer: The paper is about evaluating the performance of a language model called RAG in various tasks, including 2-way classification and generation diversity.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa({\"question\": \"What are the main topics of the paper?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HJJuMbKnz6A",
        "outputId": "0a952d50-6973-419e-8b81-93cffb879565"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is the paper about?\n",
            "Assistant: Helpful Answer: The paper is about evaluating the performance of a language model called RAG in various tasks, including 2-way classification and generation diversity.\n",
            "Follow Up Input: What are the main topics of the paper?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MapRerankDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
            "\n",
            "Question: [question here]\n",
            "Helpful Answer: [answer here]\n",
            "Score: [score between 0 and 100]\n",
            "\n",
            "How to determine the score:\n",
            "- Higher is a better answer\n",
            "- Better responds fully to the asked question, with sufficient level of detail\n",
            "- If you do not know the answer based on the context, that should be a score of 0\n",
            "- Don't be overconfident!\n",
            "\n",
            "Example #1\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Apples are red\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: red\n",
            "Score: 100\n",
            "\n",
            "Example #2\n",
            "\n",
            "Context:\n",
            "---------\n",
            "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
            "---------\n",
            "Question: what type was the car?\n",
            "Helpful Answer: a sports car or an suv\n",
            "Score: 60\n",
            "\n",
            "Example #3\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Pears are either red or orange\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: This document does not answer the question\n",
            "Score: 0\n",
            "\n",
            "Begin!\n",
            "\n",
            "Context:\n",
            "---------\n",
            "generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\n",
            "This observation suggests that the generator can complete the titles without depending on speciﬁc\n",
            "documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\n",
            "ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\n",
            "Sun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\n",
            "Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\n",
            "BART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\n",
            "with \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\n",
            "how parametric and non-parametric memories work together —the non-parametric component helps\n",
            "to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\n",
            "4.4 Fact Veriﬁcation\n",
            "---------\n",
            "Question: What are the main topics of the paper?\n",
            "Helpful Answer:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
            "\n",
            "Question: [question here]\n",
            "Helpful Answer: [answer here]\n",
            "Score: [score between 0 and 100]\n",
            "\n",
            "How to determine the score:\n",
            "- Higher is a better answer\n",
            "- Better responds fully to the asked question, with sufficient level of detail\n",
            "- If you do not know the answer based on the context, that should be a score of 0\n",
            "- Don't be overconfident!\n",
            "\n",
            "Example #1\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Apples are red\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: red\n",
            "Score: 100\n",
            "\n",
            "Example #2\n",
            "\n",
            "Context:\n",
            "---------\n",
            "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
            "---------\n",
            "Question: what type was the car?\n",
            "Helpful Answer: a sports car or an suv\n",
            "Score: 60\n",
            "\n",
            "Example #3\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Pears are either red or orange\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: This document does not answer the question\n",
            "Score: 0\n",
            "\n",
            "Begin!\n",
            "\n",
            "Context:\n",
            "---------\n",
            "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\n",
            "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
            "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
            "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
            "evidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\n",
            "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "---------\n",
            "Question: What are the main topics of the paper?\n",
            "Helpful Answer:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
            "\n",
            "Question: [question here]\n",
            "Helpful Answer: [answer here]\n",
            "Score: [score between 0 and 100]\n",
            "\n",
            "How to determine the score:\n",
            "- Higher is a better answer\n",
            "- Better responds fully to the asked question, with sufficient level of detail\n",
            "- If you do not know the answer based on the context, that should be a score of 0\n",
            "- Don't be overconfident!\n",
            "\n",
            "Example #1\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Apples are red\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: red\n",
            "Score: 100\n",
            "\n",
            "Example #2\n",
            "\n",
            "Context:\n",
            "---------\n",
            "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
            "---------\n",
            "Question: what type was the car?\n",
            "Helpful Answer: a sports car or an suv\n",
            "Score: 60\n",
            "\n",
            "Example #3\n",
            "\n",
            "Context:\n",
            "---------\n",
            "Pears are either red or orange\n",
            "---------\n",
            "Question: what color are apples?\n",
            "Helpful Answer: This document does not answer the question\n",
            "Score: 0\n",
            "\n",
            "Begin!\n",
            "\n",
            "Context:\n",
            "---------\n",
            "using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\n",
            "Machine Intelligence , 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320 .\n",
            "[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\n",
            "preprint arXiv:2002.06177 , 2020. URL https://arxiv.org/abs/2002.06177 .\n",
            "[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\n",
            "Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\n",
            "veriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\n",
            "//arxiv.org/abs/1911.03587 .\n",
            "[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\n",
            "Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\n",
            "precision training. In ICLR , 2018. URL https://openreview.net/forum?id=r1gs9JgRZ .\n",
            "[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\n",
            "---------\n",
            "Question: What are the main topics of the paper?\n",
            "Helpful Answer:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:367: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What are the main topics of the paper?',\n",
              " 'chat_history': [HumanMessage(content='What is the paper about?'),\n",
              "  AIMessage(content='Helpful Answer: The paper is about evaluating the performance of a language model called RAG in various tasks, including 2-way classification and generation diversity.'),\n",
              "  HumanMessage(content='What are the main topics of the paper?'),\n",
              "  AIMessage(content='Helpful Answer: The main topics of the paper are about the performance of the BART model in generating titles and the interaction between parametric and non-parametric memories.')],\n",
              " 'answer': 'Helpful Answer: The main topics of the paper are about the performance of the BART model in generating titles and the interaction between parametric and non-parametric memories.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtXUc7E5n67P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}